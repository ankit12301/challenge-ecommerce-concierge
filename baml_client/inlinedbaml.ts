/*************************************************************************************************

Welcome to Baml! To use this generated code, please run one of the following:

$ npm install @boundaryml/baml
$ yarn add @boundaryml/baml
$ pnpm add @boundaryml/baml

*************************************************************************************************/

// This file was generated by BAML: please do not edit it. Instead, edit the
// BAML files and re-generate this code using: baml-cli generate
// You can install baml-cli with:
//  $ npm install @boundaryml/baml
//
/* eslint-disable */
// tslint:disable
// @ts-nocheck
// biome-ignore format: autogenerated code

const fileMap = {
  
  "agent.baml": "// ============================================================================\n// AGENT DATA MODELS\n// ============================================================================\n\nenum ToolName {\n  // Product Discovery\n  SearchProducts\n  GetProductDetails\n  GetProductReviews\n  CompareProducts\n  GetRecommendations\n  \n  // Shopping Cart\n  AddToCart\n  RemoveFromCart\n  ViewCart\n  \n  // Wishlist\n  AddToWishlist\n  RemoveFromWishlist\n  ViewWishlist\n  \n  // Purchase & Orders\n  PurchaseProduct\n  Checkout\n  ViewOrders\n  \n  // No action needed\n  None\n}\n\nclass ToolCall {\n  tool ToolName\n  reasoning string @description(\"Brief explanation of why this tool is being called\")\n  parameters string @description(\"JSON string of parameters for the tool\")\n}\n\nclass AgentResponse {\n  thought string @description(\"Your internal reasoning about the user's request\")\n  action ToolCall\n  should_continue bool @description(\"True if more tool calls are needed, false if ready to respond\")\n  final_response string? @description(\"Human-friendly response to the user (required when should_continue is false)\")\n}\n\n// ============================================================================\n// MAIN AGENT FUNCTION\n// ============================================================================\n\nfunction DecideNextAction(\n  user_request: string,\n  conversation_history: string\n) -> AgentResponse {\n  client OpenRouterFree\n  prompt #\"\n    You are an intelligent e-commerce shopping assistant. Help users discover products, manage their cart and wishlist, and complete purchases.\n\n    === AVAILABLE TOOLS ===\n\n    PRODUCT DISCOVERY:\n    1. SearchProducts - Search for products by keyword\n       Parameters: {\"searchTerm\": \"string\", \"filters\": {\"minPrice\": number, \"maxPrice\": number, \"minRating\": number, \"sortBy\": \"price_asc\"|\"price_desc\"|\"rating\"|\"popularity\"}}\n       Note: filters is optional\n\n    2. GetProductDetails - Get full details about a specific product\n       Parameters: {\"productId\": \"string\"}\n\n    3. GetProductReviews - Read customer reviews for a product\n       Parameters: {\"productId\": \"string\"}\n\n    4. CompareProducts - Compare multiple products side by side\n       Parameters: {\"productIds\": [\"id1\", \"id2\", ...]}\n\n    5. GetRecommendations - Get similar product recommendations\n       Parameters: {\"productId\": \"string\", \"limit\": number}\n\n    SHOPPING CART:\n    6. AddToCart - Add a product to the shopping cart\n       Parameters: {\"productId\": \"string\", \"quantity\": number}\n\n    7. RemoveFromCart - Remove a product from the cart\n       Parameters: {\"productId\": \"string\"}\n\n    8. ViewCart - View current cart contents\n       Parameters: {}\n\n    WISHLIST:\n    9. AddToWishlist - Save a product for later\n       Parameters: {\"productId\": \"string\"}\n\n    10. RemoveFromWishlist - Remove from wishlist\n        Parameters: {\"productId\": \"string\"}\n\n    11. ViewWishlist - View saved items\n        Parameters: {}\n\n    PURCHASE & ORDERS:\n    12. PurchaseProduct - Buy a single product immediately\n        Parameters: {\"productId\": \"string\", \"quantity\": number}\n\n    13. Checkout - Purchase all items in cart\n        Parameters: {}\n\n    14. ViewOrders - View order history\n        Parameters: {}\n\n    15. None - No tool needed, just respond to the user\n        Use when you have enough information or for general conversation\n\n    === CURRENT REQUEST ===\n\n    User Request: {{ user_request }}\n\n    Conversation History:\n    {{ conversation_history }}\n\n    === GUIDELINES ===\n\n    1. SEARCH FIRST: Always search before getting details or making recommendations\n    2. BE HELPFUL: Proactively suggest related actions (e.g., \"Would you like to add this to your cart?\")\n    3. PROVIDE VALUE: When showing products, highlight key features, price comparisons, and ratings\n    4. CONFIRM PURCHASES: Always confirm before purchasing or checking out\n    5. NATURAL RESPONSES: Write final_response in friendly, conversational language\n    6. SINGLE TOOL: Only call one tool at a time, but chain multiple calls if needed\n    7. STOP WHEN DONE: Set should_continue=false when you have the information to respond\n    8. USE CONTEXT: When the user says \"buy now\", \"this\", \"it\", \"add to cart\" etc., look for [Context: ...] in the request which tells you which product they're referring to. Use that product ID.\n    9. CONTEXTUAL ACTIONS: If the user wants to buy/add/purchase \"this\" or \"it\" and context provides a product ID, use that ID directly without asking again\n\n    === RESPONSE FORMAT ===\n\n    Think step by step:\n    - What is the user asking for?\n    - What information do I have?\n    - What tool should I call next (if any)?\n    - Can I provide a helpful final response?\n\n    {{ ctx.output_format }}\n  \"#\n}\n",
  "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\n// OpenRouter clients with free models\nclient<llm> DeepSeekChimera {\n  provider openai-generic\n  options {\n    base_url \"https://openrouter.ai/api/v1\"\n    model \"tngtech/deepseek-r1t2-chimera:free\"\n    api_key env.OPENROUTER_API_KEY\n  }\n}\n\nclient<llm> KatCoderPro {\n  provider openai-generic\n  options {\n    base_url \"https://openrouter.ai/api/v1\"\n    model \"kwaipilot/kat-coder-pro:free\"\n    api_key env.OPENROUTER_API_KEY\n  }\n}\n\n// Fallback client that tries DeepSeek first, then KatCoder\nclient<llm> OpenRouterFree {\n  provider fallback\n  options {\n    strategy [DeepSeekChimera, KatCoderPro]\n  }\n}\n\n// Legacy Gemini client (commented out due to rate limits)\n// client<llm> Gemini {\n//   provider google-ai\n//   options {\n//     model \"gemini-2.0-flash-exp\"\n//     api_key env.GEMINI_API_KEY\n//   }\n// }\n\n// Using the new OpenAI Responses API for enhanced formatting\nclient<llm> OpenAIGPT4o {\n  provider openai-responses\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet4 {\n  provider anthropic\n  options {\n    model \"claude-sonnet-4-20250514\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// Example AWS Bedrock client (uncomment to use)\n// client<llm> CustomBedrock {\n//   provider aws-bedrock\n//   options {\n//     model \"anthropic.claude-sonnet-4-20250514-v1:0\"\n//     region \"us-east-1\"\n//     // AWS credentials are auto-detected from env vars\n//   }\n// }\n\n// Example Azure OpenAI client (uncomment to use)\n// client<llm> CustomAzure {\n//   provider azure-openai\n//   options {\n//     model \"gpt-5\"\n//     api_key env.AZURE_OPENAI_API_KEY\n//     base_url \"https://MY_RESOURCE_NAME.openai.azure.com/openai/deployments/MY_DEPLOYMENT_ID\"\n//     api_version \"2024-10-01-preview\"\n//   }\n// }\n\n// Example Vertex AI client (uncomment to use)\n// client<llm> CustomVertex {\n//   provider vertex-ai\n//   options {\n//     model \"gemini-2.5-pro\"\n//     location \"us-central1\"\n//     // Uses Google Cloud Application Default Credentials\n//   }\n// }\n\n// Example Ollama client for local models (uncomment to use)\n// client<llm> CustomOllama {\n//   provider openai-generic\n//   options {\n//     base_url \"http://localhost:11434/v1\"\n//     model \"llama4\"\n//     default_role \"user\" // Most local models prefer the user role\n//     // No API key needed for local Ollama\n//   }\n// }",
  "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"typescript\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.214.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode async\n}\n",
  "song_generator.baml": "// Defining a data model.\nclass Song {\n  title string\n  artist string\n  genre string\n  lyrics string\n}\n\n// Create a function to generate a song from a string.\nfunction GenerateSong(topic: string) -> Song {\n  client OpenAIGPT4o\n  prompt #\"\n    Generate a song about the following topic:\n    {{ topic }}\n\n    {{ ctx.output_format }}\n  \"#\n}",
}
export const getBamlFiles = () => {
    return fileMap;
}